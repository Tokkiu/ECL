n_layers: 2
n_heads: 2
hidden_size: 64
inner_size: 256
hidden_dropout_prob: 0.5
attn_dropout_prob: 0.5
hidden_act: 'gelu'
layer_norm_eps: 1e-12
initializer_range: 0.02
mask_ratio: 0.4
loss_type: 'CE'
generator_path: 'saved/BERT4Rec-May-11-2022_17-39-30.pth'
encoder_loss_weight: 1
discriminator_loss_weight: 1
contrastive_loss_weight: 0.5
contras_loss_temp: 0.05
pre_train: False
pre_model_path: ''
train_stage: 'alltrain'
pretrain_epochs: 20
save_step: 2
share_param: 'all'
discriminator_combine: 'cat'
discriminator_attention: 'bi'
discriminator_score_weight: 0
discriminator_score: 'add'

perturb_eps: 0.1